{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Run on GPU...\n",
      "epoch:  0 total_correct:  9618\n",
      "training accuracy:  0.19236\n",
      "testing accuracy:  0.2778\n",
      "epoch:  1 total_correct:  16418\n",
      "training accuracy:  0.32836\n",
      "testing accuracy:  0.3982\n",
      "epoch:  2 total_correct:  21607\n",
      "training accuracy:  0.43214\n",
      "testing accuracy:  0.4858\n",
      "epoch:  3 total_correct:  25460\n",
      "training accuracy:  0.5092\n",
      "testing accuracy:  0.565\n",
      "epoch:  4 total_correct:  29665\n",
      "training accuracy:  0.5933\n",
      "testing accuracy:  0.6316\n",
      "epoch:  5 total_correct:  31931\n",
      "training accuracy:  0.63862\n",
      "testing accuracy:  0.6384\n",
      "epoch:  6 total_correct:  33700\n",
      "training accuracy:  0.674\n",
      "testing accuracy:  0.6937\n",
      "epoch:  7 total_correct:  35330\n",
      "training accuracy:  0.7066\n",
      "testing accuracy:  0.7238\n",
      "epoch:  8 total_correct:  36309\n",
      "training accuracy:  0.72618\n",
      "testing accuracy:  0.7501\n",
      "epoch:  9 total_correct:  37530\n",
      "training accuracy:  0.7506\n",
      "testing accuracy:  0.7497\n",
      "epoch:  10 total_correct:  37858\n",
      "training accuracy:  0.75716\n",
      "testing accuracy:  0.7664\n",
      "epoch:  11 total_correct:  38575\n",
      "training accuracy:  0.7715\n",
      "testing accuracy:  0.7614\n",
      "epoch:  12 total_correct:  39209\n",
      "training accuracy:  0.78418\n",
      "testing accuracy:  0.7754\n",
      "epoch:  13 total_correct:  39892\n",
      "training accuracy:  0.79784\n",
      "testing accuracy:  0.7794\n",
      "epoch:  14 total_correct:  40388\n",
      "training accuracy:  0.80776\n",
      "testing accuracy:  0.7908\n",
      "epoch:  15 total_correct:  40758\n",
      "training accuracy:  0.81516\n",
      "testing accuracy:  0.8048\n",
      "epoch:  16 total_correct:  41252\n",
      "training accuracy:  0.82504\n",
      "testing accuracy:  0.8145\n",
      "epoch:  17 total_correct:  41683\n",
      "training accuracy:  0.83366\n",
      "testing accuracy:  0.8352\n",
      "epoch:  18 total_correct:  42015\n",
      "training accuracy:  0.8403\n",
      "testing accuracy:  0.824\n",
      "epoch:  19 total_correct:  42306\n",
      "training accuracy:  0.84612\n",
      "testing accuracy:  0.8307\n",
      "epoch:  20 total_correct:  42805\n",
      "training accuracy:  0.8561\n",
      "testing accuracy:  0.8293\n",
      "epoch:  21 total_correct:  42964\n",
      "training accuracy:  0.85928\n",
      "testing accuracy:  0.8373\n",
      "epoch:  22 total_correct:  43337\n",
      "training accuracy:  0.86674\n",
      "testing accuracy:  0.8434\n",
      "epoch:  23 total_correct:  43598\n",
      "training accuracy:  0.87196\n",
      "testing accuracy:  0.8491\n",
      "epoch:  24 total_correct:  43827\n",
      "training accuracy:  0.87654\n",
      "testing accuracy:  0.8428\n",
      "epoch:  25 total_correct:  44004\n",
      "training accuracy:  0.88008\n",
      "testing accuracy:  0.8541\n",
      "epoch:  26 total_correct:  44349\n",
      "training accuracy:  0.88698\n",
      "testing accuracy:  0.8529\n",
      "epoch:  27 total_correct:  44528\n",
      "training accuracy:  0.89056\n",
      "testing accuracy:  0.8537\n",
      "epoch:  28 total_correct:  44896\n",
      "training accuracy:  0.89792\n",
      "testing accuracy:  0.8608\n",
      "epoch:  29 total_correct:  45043\n",
      "training accuracy:  0.90086\n",
      "testing accuracy:  0.8605\n",
      "epoch:  30 total_correct:  45289\n",
      "training accuracy:  0.90578\n",
      "testing accuracy:  0.8514\n",
      "epoch:  31 total_correct:  45535\n",
      "training accuracy:  0.9107\n",
      "testing accuracy:  0.8571\n",
      "epoch:  32 total_correct:  45630\n",
      "training accuracy:  0.9126\n",
      "testing accuracy:  0.8528\n",
      "epoch:  33 total_correct:  45826\n",
      "training accuracy:  0.91652\n",
      "testing accuracy:  0.8539\n",
      "epoch:  34 total_correct:  46050\n",
      "training accuracy:  0.921\n",
      "testing accuracy:  0.8626\n",
      "epoch:  35 total_correct:  46248\n",
      "training accuracy:  0.92496\n",
      "testing accuracy:  0.8729\n",
      "epoch:  36 total_correct:  46361\n",
      "training accuracy:  0.92722\n",
      "testing accuracy:  0.8659\n",
      "epoch:  37 total_correct:  46514\n",
      "training accuracy:  0.93028\n",
      "testing accuracy:  0.8649\n",
      "epoch:  38 total_correct:  46589\n",
      "training accuracy:  0.93178\n",
      "testing accuracy:  0.8626\n",
      "epoch:  39 total_correct:  46870\n",
      "training accuracy:  0.9374\n",
      "testing accuracy:  0.8613\n",
      "epoch:  40 total_correct:  46934\n",
      "training accuracy:  0.93868\n",
      "testing accuracy:  0.8753\n",
      "epoch:  41 total_correct:  47008\n",
      "training accuracy:  0.94016\n",
      "testing accuracy:  0.8783\n",
      "epoch:  42 total_correct:  47254\n",
      "training accuracy:  0.94508\n",
      "testing accuracy:  0.8793\n",
      "epoch:  43 total_correct:  47476\n",
      "training accuracy:  0.94952\n",
      "testing accuracy:  0.8754\n",
      "epoch:  44 total_correct:  47498\n",
      "training accuracy:  0.94996\n",
      "testing accuracy:  0.8785\n",
      "epoch:  45 total_correct:  47625\n",
      "training accuracy:  0.9525\n",
      "testing accuracy:  0.8751\n",
      "epoch:  46 total_correct:  47831\n",
      "training accuracy:  0.95662\n",
      "testing accuracy:  0.8858\n",
      "epoch:  47 total_correct:  47954\n",
      "training accuracy:  0.95908\n",
      "testing accuracy:  0.8887\n",
      "epoch:  48 total_correct:  48057\n",
      "training accuracy:  0.96114\n",
      "testing accuracy:  0.8779\n",
      "epoch:  49 total_correct:  48034\n",
      "training accuracy:  0.96068\n",
      "testing accuracy:  0.8809\n",
      "0.8809\n",
      "[0.8809]\n",
      "        0\n",
      "0  0.8809\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "# import os\n",
    "import shutil\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from prune_layer import *\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            PruneLinear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            PruneLinear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            PruneLinear(512, 10),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = PrunedConv(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def get_num_correct(pred,labels):\n",
    "    return pred.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m)==nn.Linear or type(m)==nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def Net():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "    \n",
    "    \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "])\n",
    "train_set=torchvision.datasets.CIFAR10(\n",
    "    root='~/work/data/Xian/cifar10',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train)\n",
    "\n",
    "\n",
    "test_set=torchvision.datasets.CIFAR10(\n",
    "    root='~/work/data/Xian/cifar10',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device =='cuda':\n",
    "    print(\"Run on GPU...\")\n",
    "else:\n",
    "    print(\"Run on CPU...\")\n",
    "\n",
    "def train_(train_set,test_set,layer_name, q, PATH = \"sdfadsfag.pt\", batch_size_train = 128, momentum = 0, mask_bit_position = [1]*16, lr = 0.1, epochs = 10):\n",
    "    torch.manual_seed(1)\n",
    "    train_loader=torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=False, pin_memory=True,num_workers=2)\n",
    "    test_loader=torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, pin_memory=True,num_workers=2)\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    network= Net().to(device)\n",
    "    network.apply(init_weights)\n",
    "    layer_num =0\n",
    "    for n, m in network.named_modules():\n",
    "        if isinstance(m,PrunedConv):\n",
    "            if layer_num in layer_name:\n",
    "                 m.prune_by_percentage(q = q)\n",
    "            layer_num +=1\n",
    "        if isinstance(m,PruneLinear):\n",
    "            if layer_num in layer_name:\n",
    "                 m.prune_by_percentage(q = q)\n",
    "            layer_num +=1                \n",
    "\n",
    "    optimizer = optim.SGD(network.parameters(), lr=lr, momentum = 0.9, weight_decay=0.0005)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    acc_train=[]\n",
    "    acc_test=[]\n",
    "    acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        network.train()\n",
    "        count_in = 0\n",
    "\n",
    "        for batch in train_loader: #Get batch\n",
    "\n",
    "            count_in = count_in + 1\n",
    "            images,labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Do bit maskinn\n",
    "            layer_num  = 0\n",
    "            for name, model in network.named_modules():\n",
    "                if isinstance(model, PrunedConv):\n",
    "                    if layer_num in layer_name:\n",
    "                        model.conv.weight.data.mul_(model.mask)\n",
    "                    layer_num +=1\n",
    "                elif isinstance(model, PruneLinear):\n",
    "                    if layer_num in layer_name:\n",
    "                        model.linear.weight.data.mul_(model.mask)\n",
    "                    layer_num +=1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            preds=network(images) #pass batch to network\n",
    "            correct = get_num_correct(preds, labels)\n",
    "            loss = criterion(preds,labels) #Calculate loss\n",
    "            loss.backward() #Calculate gradients\n",
    "            optimizer.step() #Update weights\n",
    "            total_correct+=correct\n",
    "            \n",
    "        print(\"epoch: \", epoch,  \"total_correct: \", total_correct)\n",
    "        print(\"training accuracy: \", total_correct/len(train_set))\n",
    "        acc_train.append(deepcopy(float(total_correct)/len(train_set)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_test=0\n",
    "            for batch_test in test_loader: #Get batch\n",
    "                images_test,labels_test = batch_test\n",
    "                images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "                preds_test=network(images_test) #pass batch to network\n",
    "                correct_test += get_num_correct(preds_test, labels_test)\n",
    "            print(\"testing accuracy: \", correct_test / len(test_set))\n",
    "            if epoch == epochs - 1:\n",
    "                print(correct_test / len(test_set))\n",
    "                acc = correct_test / len(test_set)\n",
    "                torch.save(network.state_dict(), PATH)\n",
    "                  \n",
    "            acc_test.append(deepcopy(float(correct_test)/len(test_set)))\n",
    "        scheduler.step()\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_q = [5]\n",
    "list_acc = []\n",
    "list_layers =[9]\n",
    "for layer_name in list_layers:\n",
    "    for q in list_q:\n",
    "        acc = train_(train_set,test_set,PATH = \"VGG11_lr01_layer_9_q1\",layer_name=[layer_name],lr = 0.1, q=q,epochs = 50)\n",
    "        list_acc += [acc]\n",
    "    print(list_acc)\n",
    "        \n",
    "import pandas as pd\n",
    "list_acc = np.array(list_acc).reshape((len(list_layers),-1))\n",
    "df = pd.DataFrame (list_acc)\n",
    "print(df)\n",
    "## save to xlsx file\n",
    "# filepath = './results/VGG11_weight_jlab.csv'\n",
    "\n",
    "# df.to_csv(filepath,index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "network= Net().to(device)\n",
    "network.load_state_dict(torch.load(\"VGG11_lr01_layer_7_q1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_name = [1]\n",
    "layer_num = 1\n",
    "for name, model in network.named_modules():\n",
    "    if isinstance(model, PrunedConv):\n",
    "        if layer_num in layer_name:\n",
    "            weight = model.conv.weight.data.cpu().detach().numpy().reshape(-1)\n",
    "            _ = plt.hist(weight, bins=20)\n",
    "        layer_num +=1\n",
    "    elif isinstance(model, PruneLinear):\n",
    "        if layer_num in layer_name:\n",
    "            weight = model.linear.weight.data.cpu().detach().numpy().reshape(-1)\n",
    "            _ = plt.hist(weight, bins=20)\n",
    "        layer_num +=1\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
